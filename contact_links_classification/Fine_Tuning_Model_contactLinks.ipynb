{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85780b87-6516-4dda-b3ff-fe7587fafc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02e759e-3233-44c9-98d1-602875250fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactLinkModel:\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-cased', num_labels=2, max_length=40):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.linkProcessing = LinkProcessing()\n",
    "        \n",
    "    def load_from_huggingface(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels= self.num_labels\n",
    "        )\n",
    "\n",
    "    def load_from_local(self, tokenizer_path='bert-base-cased', model_path='./Models/model_3/model_contact_40_maxlen_30_epochs'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    def preprocess(self, texts, truncation=True, padding=True):\n",
    "        return self.tokenizer(\n",
    "            texts, \n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def get_original_tokens(self, input_ids):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return tokens\n",
    "\n",
    "    def compute_metrics(self, preds, labels):\n",
    "        preds = preds.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        recall = recall_score(labels, preds, average='binary')\n",
    "        precision = precision_score(labels, preds, average='binary')\n",
    "        f1 = f1_score(labels, preds, average='binary')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels, num_epochs, batch_size):\n",
    "        \n",
    "        os.makedirs(\"./Models/model_3\", exist_ok=True)\n",
    "        model_save_path = f\"./Models/model_3/model_contact_{self.max_length}_maxlen_{num_epochs}_epochs\"\n",
    "\n",
    "        train_encodings = self.preprocess(train_texts)\n",
    "        val_encodings = self.preprocess(val_texts)\n",
    "\n",
    "        train_dataset = Dataset(train_encodings, train_labels)\n",
    "        val_dataset = Dataset(val_encodings, val_labels)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "        csv_filename = f\"{model_save_path}_info.csv\"\n",
    "        header = [\"training_details\"]\n",
    "        is_empty = True\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_preds, val_labels = [], []\n",
    "            val_total_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    logits = outputs.logits\n",
    "                    val_preds.extend(logits.detach().cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    loss = outputs.loss\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_loader)\n",
    "\n",
    "            val_preds = np.array(val_preds)\n",
    "            val_labels = np.array(val_labels)\n",
    "            accuracy, precision, recall, f1 = self.compute_metrics(val_preds, val_labels)\n",
    "\n",
    "            training_details = (f\"Époque {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - \"\n",
    "                                f\"Validation Loss: {avg_val_loss:.4f} - Validation Accuracy: {accuracy:.4f} - \"\n",
    "                                f\"Precision: {precision:.4f} - Recall: {recall:.4f} - F1 Score: {f1:.4f}\")\n",
    "            print(training_details)\n",
    "        \n",
    "            with open(csv_filename, 'a', newline='', encoding='utf-8-sig') as csvfile:\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                if is_empty:\n",
    "                    csv_writer.writerow(header)\n",
    "                    is_empty = False\n",
    "                csv_writer.writerow([training_details])\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_fine_tuning_time = end_time - start_time\n",
    "        \n",
    "        training_details = f\"Fine-tuning terminé! Temps total: {total_fine_tuning_time:.2f} secondes | {total_fine_tuning_time/60:.2f} min | {total_fine_tuning_time/3600:.2f} hours\"\n",
    "        print(training_details)\n",
    "\n",
    "        # Enregistrer le modèle finetuné\n",
    "        self.model.save_pretrained(model_save_path)\n",
    "        print(f\"Modèle enregistré à {model_save_path}\")\n",
    "        \n",
    "        with open(csv_filename, 'a', newline='', encoding='utf-8-sig') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow([training_details])\n",
    "            csv_writer.writerow([f\"Modèle enregistré à {model_save_path}\"])\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.preprocess([text])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        predictions = np.argmax(predictions)\n",
    "        return predictions\n",
    "\n",
    "    def predict_label_links(self, cleaned_links):\n",
    "        link_name_label = []\n",
    "        for i in range(len(cleaned_links)):\n",
    "            predictions = self.predict(cleaned_links[i][1])\n",
    "            link_name_label.append((cleaned_links[i][0], cleaned_links[i][1], predictions))\n",
    "        return link_name_label\n",
    "        \n",
    "    def get_contact_links(self, htmlContent):\n",
    "        links = self.linkProcessing.preprocess_links(htmlContent)\n",
    "        predictedLinks = self.predict_label_links(links)\n",
    "        return [link for link in predictedLinks if link[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2b3c84-675f-400f-af28-761592f8b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec2e248-075b-465b-b90e-8046b73a737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkProcessing :\n",
    "    def __init__(self):\n",
    "        self.max_len_link_name = 6\n",
    "        \n",
    "    def extract_links(self, contenu_html):\n",
    "        soup = BeautifulSoup(contenu_html, 'html.parser')\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            text = link.get_text(strip=True)\n",
    "            links.append((href, text))\n",
    "        return links\n",
    "\n",
    "    def remove_empty_links(self, links):\n",
    "        cleaned_links = [(href, text) for href, text in links if href.strip() not in (\"#\", \"\")]\n",
    "        return cleaned_links\n",
    "\n",
    "    def filter_valid_name_links(self, links):\n",
    "        cleaned_links = [(href, text) for href, text in links if text.strip() and len(text.split()) <= self.max_len_link_name]\n",
    "        return cleaned_links\n",
    "\n",
    "    def preprocess_links(self, contenu_html):\n",
    "        links = self.extract_links(contenu_html)\n",
    "        cleaned_links = self.remove_empty_links(links)\n",
    "        cleaned_links = self.filter_valid_name_links(cleaned_links)\n",
    "        return cleaned_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d65c0e-457a-44f0-9680-2fdf2c9d2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/LINK_CONTACT_DATA_2.csv')\n",
    "# data = data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db5adec-ce14-4a7c-b9f6-621cfc72c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"link_name\"])\n",
    "y = list(data[\"label\"])\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302b645a-413f-4ac0-b164-1bbd4b457290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_name = 'bert-base-cased'\n",
    "num_labels = 2\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6970b671-5c70-45b7-89fe-dfd02e10700f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "classifier = ContactLinkModel(model_name, num_labels, max_length)\n",
    "classifier.load_from_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cabf83-1518-473d-a17b-a1263ef5dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 1/30 - Train Loss: 0.1462 - Validation Loss: 0.0351 - Validation Accuracy: 0.9900 - Precision: 0.9896 - Recall: 0.9896 - F1 Score: 0.9896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 2/30 - Train Loss: 0.0376 - Validation Loss: 0.0486 - Validation Accuracy: 0.9859 - Precision: 0.9776 - Recall: 0.9938 - F1 Score: 0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 3/30 - Train Loss: 0.0244 - Validation Loss: 0.0404 - Validation Accuracy: 0.9910 - Precision: 0.9938 - Recall: 0.9876 - F1 Score: 0.9907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 4/30 - Train Loss: 0.0222 - Validation Loss: 0.0266 - Validation Accuracy: 0.9930 - Precision: 0.9938 - Recall: 0.9917 - F1 Score: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 5/30 - Train Loss: 0.0185 - Validation Loss: 0.0346 - Validation Accuracy: 0.9920 - Precision: 0.9917 - Recall: 0.9917 - F1 Score: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 6/30 - Train Loss: 0.0131 - Validation Loss: 0.0277 - Validation Accuracy: 0.9930 - Precision: 0.9938 - Recall: 0.9917 - F1 Score: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 7/30 - Train Loss: 0.0086 - Validation Loss: 0.0437 - Validation Accuracy: 0.9920 - Precision: 0.9917 - Recall: 0.9917 - F1 Score: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_800923/3573251049.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "classifier.train(X_train, y_train, X_val, y_val, num_epochs=30, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef92156-a098-45f0-9b57-3a8447bbfdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869e7e3-ca5d-4371-9285-99210a0ea665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26ae43-b887-467b-b0ec-5823edb768b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16612072-dad1-40e6-ae1f-e62bdf87bd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bcf96a-1621-432a-93bb-d66d5e00ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('https://www.hespress.com/contact1', 'اعمل معنا'),\n",
       " ('https://www.hespress.com/contact2', 'contact'),\n",
       " ('https://www.hespress.com/contact2', 'Informations de contact'),\n",
       " ('https://www.hespress.com/contact2', 'هيئة التحرير'),\n",
       " ('https://www.hespress.com/contact2', 'من نحن؟'),\n",
       " ('https://www.hespress.com/contact2', 'اتصال')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ContactLinkModel import ContactLinkModel\n",
    "\n",
    "classifier = ContactLinkModel()\n",
    "classifier.load_from_local()\n",
    "\n",
    "text = \"\"\"<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div>اعمل معنا<div></a>\n",
    "<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div><div></a>\n",
    "<a class=\"nav-link\" href=\"#\"><div>aaa<div></a>\n",
    "<a class=\"nav-link\" href=\"https://www.hespress.com/contact1\"><div>bbb<div></a>\n",
    "        <li class=\"menu-item nav-item\"><a class=\"nav-link\" href=\"https://jdjd/jd\">cc cc cc cc cc cc cc cc</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">hhh</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">contact</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">Informations de contact</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">هيئة التحرير</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">من نحن؟</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">اتصال</a>\n",
    "        <a class=\"nav-link\" href=\"https://www.hespress.com/contact2\">economie</a>\n",
    "        </li>\"\"\"\n",
    "\n",
    "test = classifier.get_contact_links(text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb04bc-a351-4225-8a6f-6ec3ac035d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74ae26-02cd-4439-ab2a-4f2248553920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
